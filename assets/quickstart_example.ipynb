{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Vistral-V: A Vietnamese Large Vision-Language Model\n","\n","This project demonstrates visual instruction tuning applied to the Vistral large language model (LLM). This enables the model to understand and respond to prompts that combine both visual and textual information in Vietnamese.\n","\n","**Key features:**\n","\n","* **Multimodal understanding:** Process images and Vietnamese text together.\n","* **Instruction following:** Respond to complex instructions that involve visual input.\n","* **Vietnamese language specialization:** Fine-tuned for accuracy and fluency in Vietnamese.\n","\n","**Resources:**\n","\n","* **GitHub repository:** [Vistral-V](https://github.com/hllj/Vistral-V/tree/main?tab=readme-ov-file) (Codebase and details)\n","* **HuggingFace model:** [Vistral-V 7B](https://huggingface.co/Vi-VLM/Vistral-V-7B) (Try it out!)\n","\n","\n","> **Access Requirement**: The following demonstration employs the [Vistral Vision model](https://huggingface.co/Vi-VLM/Vistral-V-7B), which requires authorized access. Please ensure you have the necessary permissions on Hugging Face to interact with the model before proceeding."]},{"cell_type":"markdown","metadata":{},"source":["## Login HuggingFace"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-29T18:11:25.802473Z","iopub.status.busy":"2024-06-29T18:11:25.802104Z","iopub.status.idle":"2024-06-29T18:11:26.309749Z","shell.execute_reply":"2024-06-29T18:11:26.308763Z","shell.execute_reply.started":"2024-06-29T18:11:25.802432Z"},"trusted":true},"outputs":[],"source":["# Add Read HuggingFace Token to download weight models\n","from kaggle_secrets import UserSecretsClient\n","from huggingface_hub.hf_api import HfFolder\n","\n","# Get HF Token, please add HF read token to Kaggle secret, or token = \"hf...\"\n","token = UserSecretsClient().get_secret(\"HF_TOKEN\") \n","\n","HfFolder.save_token(token)"]},{"cell_type":"markdown","metadata":{},"source":["## Environment Setup\n","\n","1. Git clone our repo\n","2. Install the customized package (which supports best for the model)\n","3. Install other requirements from pip"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-29T18:11:26.311068Z","iopub.status.busy":"2024-06-29T18:11:26.310798Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'Vistral-V' already exists and is not an empty directory.\n","/kaggle/working/Vistral-V\n","Obtaining file:///kaggle/working/Vistral-V\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n","\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: torch==2.1.2 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (2.1.2)\n","Requirement already satisfied: torchvision==0.16.2 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.16.2)\n","Collecting transformers==4.37.2 (from llava==1.2.2.post1)\n","  Using cached transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n","Collecting tokenizers==0.15.1 (from llava==1.2.2.post1)\n","  Using cached tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.1.99)\n","Requirement already satisfied: shortuuid in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (1.0.13)\n","Requirement already satisfied: accelerate==0.21.0 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.21.0)\n","Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.11.1)\n","Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.43.1)\n","Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (2.5.3)\n","Requirement already satisfied: markdown2[all] in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (2.4.13)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (1.26.4)\n","Requirement already satisfied: scikit-learn==1.2.2 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (1.2.2)\n","Requirement already satisfied: gradio==4.16.0 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (4.16.0)\n","Requirement already satisfied: gradio-client==0.8.1 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.8.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (2.32.3)\n","Requirement already satisfied: httpx==0.24.0 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.24.0)\n","Requirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.25.0)\n","Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.108.0)\n","Requirement already satisfied: einops==0.6.1 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.6.1)\n","Requirement already satisfied: einops-exts==0.0.4 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.0.4)\n","Requirement already satisfied: timm==0.6.13 in /opt/conda/lib/python3.10/site-packages (from llava==1.2.2.post1) (0.6.13)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->llava==1.2.2.post1) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->llava==1.2.2.post1) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0->llava==1.2.2.post1) (6.0.1)\n","Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (22.1.0)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (5.3.0)\n","Requirement already satisfied: ffmpy in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (0.3.2)\n","Requirement already satisfied: huggingface-hub>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (0.23.2)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (6.1.1)\n","Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (3.1.2)\n","Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (2.1.3)\n","Requirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (3.7.5)\n","Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (3.9.10)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (2.2.1)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (9.5.0)\n","Requirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (0.25.1)\n","Requirement already satisfied: python-multipart in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (0.0.9)\n","Requirement already satisfied: ruff>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (0.5.0)\n","Requirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (2.10.0)\n","Requirement already satisfied: tomlkit==0.12.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (0.12.0)\n","Requirement already satisfied: typer<1.0,>=0.9 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio==4.16.0->llava==1.2.2.post1) (0.9.0)\n","Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.16.0->llava==1.2.2.post1) (4.9.0)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==0.8.1->llava==1.2.2.post1) (2024.3.1)\n","Requirement already satisfied: websockets<12.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==0.8.1->llava==1.2.2.post1) (11.0.3)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx==0.24.0->llava==1.2.2.post1) (2024.2.2)\n","Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from httpx==0.24.0->llava==1.2.2.post1) (0.17.3)\n","Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx==0.24.0->llava==1.2.2.post1) (3.6)\n","Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx==0.24.0->llava==1.2.2.post1) (1.3.0)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->llava==1.2.2.post1) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->llava==1.2.2.post1) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->llava==1.2.2.post1) (3.2.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->llava==1.2.2.post1) (3.13.1)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->llava==1.2.2.post1) (1.12.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->llava==1.2.2.post1) (3.2.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.2->llava==1.2.2.post1) (2023.12.25)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.2->llava==1.2.2.post1) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.37.2->llava==1.2.2.post1) (4.66.4)\n","Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->llava==1.2.2.post1) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic->llava==1.2.2.post1) (2.14.6)\n","Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->llava==1.2.2.post1) (8.1.7)\n","Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn->llava==1.2.2.post1) (0.14.0)\n","Requirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->llava==1.2.2.post1) (0.32.0.post1)\n","Requirement already satisfied: pygments>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from markdown2[all]->llava==1.2.2.post1) (2.17.2)\n","Requirement already satisfied: wavedrom in /opt/conda/lib/python3.10/site-packages (from markdown2[all]->llava==1.2.2.post1) (2.0.3.post3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->llava==1.2.2.post1) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->llava==1.2.2.post1) (1.26.18)\n","Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (4.20.0)\n","Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (0.12.1)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx==0.24.0->llava==1.2.2.post1) (4.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.16.0->llava==1.2.2.post1) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==4.16.0->llava==1.2.2.post1) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==4.16.0->llava==1.2.2.post1) (2023.4)\n","Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio==4.16.0->llava==1.2.2.post1) (0.4.6)\n","Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio==4.16.0->llava==1.2.2.post1) (1.5.4)\n","Requirement already satisfied: rich<14.0.0,>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio==4.16.0->llava==1.2.2.post1) (13.7.0)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.2->llava==1.2.2.post1) (1.3.0)\n","Requirement already satisfied: svgwrite in /opt/conda/lib/python3.10/site-packages (from wavedrom->markdown2[all]->llava==1.2.2.post1) (1.4.3)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from wavedrom->markdown2[all]->llava==1.2.2.post1) (1.16.0)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx==0.24.0->llava==1.2.2.post1) (1.2.0)\n","Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (0.32.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.16.0->llava==1.2.2.post1) (0.16.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.16.0->llava==1.2.2.post1) (3.0.0)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.16.0->llava==1.2.2.post1) (0.1.2)\n","Using cached tokenizers-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","Using cached transformers-4.37.2-py3-none-any.whl (8.4 MB)\n","Building wheels for collected packages: llava\n","  Building editable for llava (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for llava: filename=llava-1.2.2.post1-0.editable-py3-none-any.whl size=10319 sha256=caf859bfe410954379eb1b1b8191bbb25aaf956f35b498831a59722f354a07df\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-tg4d7f6u/wheels/03/f8/b5/2c1390454b11460afdf3d56cfebcffc5e58de711b43b26ab9c\n","Successfully built llava\n","Installing collected packages: tokenizers, transformers, llava\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.19.1\n","    Uninstalling tokenizers-0.19.1:\n","      Successfully uninstalled tokenizers-0.19.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.41.2\n","    Uninstalling transformers-4.41.2:\n","      Successfully uninstalled transformers-4.41.2\n","  Attempting uninstall: llava\n","    Found existing installation: llava 1.2.2.post1\n","    Uninstalling llava-1.2.2.post1:\n","      Successfully uninstalled llava-1.2.2.post1\n","Successfully installed llava-1.2.2.post1 tokenizers-0.15.1 transformers-4.37.2\n"]}],"source":["# Clone the repository and install the packages of vistral-V\n","!git clone https://github.com/hllj/Vistral-V.git\n","%cd Vistral-V\n","!pip install -e .\n","\n","# Install customized transformers\n","!pip install transformers==4.41.2 -q\n","\n","# Install stuff\n","!pip install ipykernel -q\n","!pip install ipywidgets -q\n","!jupyter nbextension enable --py widgetsnbextension"]},{"cell_type":"markdown","metadata":{},"source":["## Getting Started\n","\n","This Kaggle example demonstrates how to use [Vistral Vision](https://huggingface.co/Vi-VLM/Vistral-V-7B), an AI model that can understand and discuss images in Vietnamese. \n","\n","*Note*: To run this code, you'll need access to the model on Hugging Face. If you don't have access yet, please request it before proceeding.\n","\n","\n","**What you'll see:**\n","* **Sets up the Vistral Vision model:** We'll load the model and get it ready to process images and text.\n","* **Interactive conversations:**  Ask questions about images, and Vistral Vision will respond in Vietnamese.\n","\n","**Try it yourself!** Follow the steps below to run the code and start a conversation with Vistral Vision."]},{"cell_type":"markdown","metadata":{},"source":["### Setting configs"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Option 1: Lora Adapter and Base model\n","# model_path = \"Vi-VLM/llava-vistral-7b-lora\"\n","# model_base = \"Viet-Mistral/Vistral-7B-Chat\"\n","\n","# Option 2: Full Model\n","\n","model_path = \"Vi-VLM/Vistral-V-7B\"\n","model_base = None\n","\n","conv_mode = \"vistral\"\n","\n","image_file = \"assets/images/example.jpeg\"\n","\n","temperature = 0.2\n","max_new_tokens = 512\n","load_8bit = False\n","load_4bit = False\n","\n","debug = False\n","device = \"cuda:0\" # device = \"cuda\" if you want to inference with multiple GPU devices"]},{"cell_type":"markdown","metadata":{},"source":["### Import Libraries and Load the Model"]},{"cell_type":"markdown","metadata":{},"source":["To ensure optimal model performance and compatibility in diverse environments, we enable `disable_torch_init`. This helps prevent conflicts and improves resource management when deploying the model."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from llava.utils import disable_torch_init\n","\n","disable_torch_init()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from llava.model.builder import load_pretrained_model\n","from llava.mm_utils import get_model_name_from_path\n","\n","# Get the model name\n","model_name = get_model_name_from_path(model_path)\n","\n","# FIXME: Vistral-V is need to be fixed, add prefix 'llava-'\n","if 'vistral-v' in model_name.lower():\n","    model_name = 'llava-' + model_name\n","\n","# Load model, tokenizer and processor stuff\n","tokenizer, model, image_processor, context_len = load_pretrained_model(\n","    model_path=model_path,\n","    model_base=model_base,\n","    model_name=model_name,\n","    load_8bit=load_8bit,\n","    load_4bit=load_4bit,\n","    device=device\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Load image using"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import requests\n","from PIL import Image\n","from io import BytesIO\n","\n","def load_image(image_file):\n","    if image_file.startswith('http://') or image_file.startswith('https://'):\n","        response = requests.get(image_file)\n","        image = Image.open(BytesIO(response.content)).convert('RGB')\n","    else:\n","        image = Image.open(image_file).convert('RGB')\n","    return image\n","\n","image = load_image(image_file)\n","\n","print(\"This is the image using\")\n","display(image)"]},{"cell_type":"markdown","metadata":{},"source":["### Configuring the Conversation Template"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from llava.conversation import conv_templates\n","\n","if \"llama-2\" in model_name.lower():\n","    conv_mode = \"llava_llama_2\"\n","elif \"mistral\" in model_name.lower():\n","    conv_mode = \"mistral_instruct\"\n","elif \"vistral\" in model_name.lower():\n","    conv_mode = \"vistral\"\n","elif \"v1.6-34b\" in model_name.lower():\n","    conv_mode = \"chatml_direct\"\n","elif \"v1\" in model_name.lower():\n","    conv_mode = \"llava_v1\"\n","elif \"mpt\" in model_name.lower():\n","    conv_mode = \"mpt\"\n","else:\n","    conv_mode = \"llava_v0\"\n","\n","conv = conv_templates[conv_mode].copy()\n","if \"mpt\" in model_name.lower():\n","    roles = ('user', 'assistant')\n","else:\n","    roles = conv.roles"]},{"cell_type":"markdown","metadata":{},"source":["## Let's chat !!"]},{"cell_type":"markdown","metadata":{},"source":["### Set Up Your Workspace\n","Start by importing the required libraries and loading the image you want to analyze.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","\n","from llava.conversation import SeparatorStyle\n","from llava.mm_utils import process_images, tokenizer_image_token\n","from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n","\n","from transformers import TextStreamer\n","from IPython.display import display\n","\n","# Get the image size and díplay it\n","image_size = image.size\n","\n","# Similar operation in model_worker.py\n","image_tensor = process_images([image], image_processor, model.config)\n","if type(image_tensor) is list:\n","    image_tensor = [image.to(model.device, dtype=torch.float16) for image in image_tensor]\n","else:\n","    image_tensor = image_tensor.to(model.device, dtype=torch.float16)"]},{"cell_type":"markdown","metadata":{},"source":["### Chat !"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["while True:\n","    try:\n","        inp = input(f\"{roles[0]}: \")\n","    except EOFError:\n","        inp = \"\"\n","    if not inp:\n","        print(\"exit...\")\n","        break\n","\n","    print(f\"{roles[1]}: \", end=\"\")\n","\n","    if image is not None:\n","        # first message\n","        if model.config.mm_use_im_start_end:\n","            inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + inp\n","        else:\n","            inp = DEFAULT_IMAGE_TOKEN + '\\n' + inp\n","        image = None\n","\n","    conv.append_message(conv.roles[0], inp)\n","    conv.append_message(conv.roles[1], None)\n","    prompt = conv.get_prompt()\n","\n","    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\n","    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n","    keywords = [stop_str]\n","    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n","\n","    with torch.inference_mode():\n","        output_ids = model.generate(\n","            input_ids,\n","            images=image_tensor,\n","            image_sizes=[image_size],\n","            do_sample=True if temperature > 0 else False,\n","            temperature=temperature,\n","            max_new_tokens=max_new_tokens,\n","            streamer=streamer,\n","            use_cache=True)\n","\n","    outputs = tokenizer.decode(output_ids[0]).strip()\n","    conv.messages[-1][-1] = outputs\n","\n","    if debug:\n","        print(\"\\n\", {\"prompt\": prompt, \"outputs\": outputs}, \"\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
